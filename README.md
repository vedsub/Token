# Token ğŸ¤–

A collection of LLM prompting techniques and API integrations demonstrating various approaches to working with AI models including Google Gemini and Ollama.

## ğŸ“‹ Overview

This project showcases different prompting strategies and LLM integration patterns:

- **Zero-Shot Prompting** - Direct prompting without examples
- **Chain of Thought (CoT) Prompting** - Step-by-step reasoning approach
- **Few-Shot Prompting** - Learning from examples
- **Local LLM API Server** - FastAPI server using Ollama with Gemma 3

## ğŸ—‚ï¸ Project Structure

```
tokenise/
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ zero.py        # Zero-shot prompting example
â”‚   â”œâ”€â”€ cot.py         # Chain of Thought with interactive chat
â”‚   â””â”€â”€ few.py         # Few-shot prompting (WIP)
â”œâ”€â”€ ollama-fastapi/
â”‚   â”œâ”€â”€ server.py      # FastAPI server for Ollama
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ main.py            # Basic Gemini API usage
â”œâ”€â”€ gemini.py          # Gemini via OpenAI-compatible API
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env               # API keys (not tracked)
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.ai/) installed locally (for local LLM features)
- Google Gemini API key (for cloud features)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/vedsub/Token.git
   cd Token
   ```

2. **Create and activate virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env and add your API keys
   ```

### Running Ollama (for local LLM)

1. **Start Ollama server**
   ```bash
   ollama serve
   ```

2. **Pull the Gemma 3 model**
   ```bash
   ollama pull gemma3:270m
   ```

## ğŸ’¡ Usage

### Chain of Thought Interactive Chat

Run an interactive chat session with CoT reasoning:

```bash
python prompts/cot.py
```

The assistant will break down problems using the **Start â†’ Plan â†’ Action â†’ Output** format.

### Zero-Shot Prompting

```bash
python prompts/zero.py
```

### Ollama FastAPI Server

Start the local API server:

```bash
cd ollama-fastapi
uvicorn server:app --reload
```

**Endpoints:**
- `GET /` - Health check
- `POST /chat` - Send a message to Gemma 3

**Example request:**
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '"Hello, how are you?"'
```

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Cloud LLM | Google Gemini 2.5 Flash |
| Local LLM | Ollama with Gemma 3 (270M) |
| API Framework | FastAPI |
| Python SDK | OpenAI, Google GenAI |

## ğŸ“š Prompting Techniques

### Zero-Shot Prompting
Direct queries without examples. Good for straightforward tasks where the model's pre-training is sufficient.

### Chain of Thought (CoT)
Encourages step-by-step reasoning:
1. **Start** - Understand the problem
2. **Plan** - Outline the approach
3. **Action** - Execute the plan
4. **Output** - Provide the final answer

### Few-Shot Prompting
Provides examples to guide the model's responses (coming soon).

## ğŸ“ Environment Variables

Create a `.env` file with:

```env
GEMINI_API_KEY=your_gemini_api_key_here
```

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests!

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).
